{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "import spacy\n",
    "\n",
    "import re \n",
    "import time\n",
    "import pickle\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Probably because it doesn't taste like coffee....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Stash have tried very hard to get quality prod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>this a great snack... the spicy taste is a kic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>One of the biggest challenge to mix more than ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>I liked the tree, but when it arrived, I felt ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                               Text\n",
       "0      1  Probably because it doesn't taste like coffee....\n",
       "1      4  Stash have tried very hard to get quality prod...\n",
       "2      4  this a great snack... the spicy taste is a kic...\n",
       "3      3  One of the biggest challenge to mix more than ...\n",
       "4      2  I liked the tree, but when it arrived, I felt ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewdf = pd.read_csv(\"./data/Reviews.csv\", usecols=[\"Score\",\"Text\"])\n",
    "\n",
    "#Make all review scores equal distribution by undersampling\n",
    "balanced = None\n",
    "for i in reviewdf[\"Score\"].unique():\n",
    "    balanced = pd.concat([balanced, reviewdf[reviewdf[\"Score\"] == i][0:29500]])\n",
    "balanced = balanced.sample(frac=1).reset_index(drop=True)\n",
    "del reviewdf\n",
    "balanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(x):\n",
    "    x = re.sub('\\\\\\\\|<[^>]+>', '', x) #remove <br>\n",
    "    x = re.sub(r'\\([^)]*\\)', '', x) #remove (in-between parenthesis)\n",
    "    x = x.replace('\"',\"\")\n",
    "    x = x.replace(\"'\",\"\")\n",
    "    return x \n",
    "\n",
    "balanced[\"Text\"] = balanced[\"Text\"].apply(lambda x: clean_review(x))\n",
    "balanced[0:29500*4].to_csv('amazon_train.csv', index=False)\n",
    "balanced[29500*4:].to_csv('amazon_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading spacy.\n",
      "Finished loading data.\n"
     ]
    }
   ],
   "source": [
    "SCORE = data.LabelField(dtype=torch.long)\n",
    "REVIEW = data.Field(tokenize='spacy', lower=True, include_lengths=True)\n",
    "print('Finished loading spacy.')\n",
    "\n",
    "fields = [('score', SCORE), ('review', REVIEW)]\n",
    "\n",
    "train_data, test_data = data.TabularDataset.splits(\n",
    "                                        path = '.',\n",
    "                                        train = 'amazon_train.csv',\n",
    "                                        test = 'amazon_test.csv',\n",
    "                                        format = 'csv',\n",
    "                                        fields = fields,\n",
    "                                        skip_header = True\n",
    ")\n",
    "train_data, valid_data = train_data.split(split_ratio=0.8)\n",
    "print('Finished loading data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "batch_size = 100\n",
    "\n",
    "REVIEW.build_vocab(train_data, max_size = vocab_size, vectors=\"glove.6B.100d\", unk_init = torch.Tensor.normal_)\n",
    "SCORE.build_vocab(train_data)\n",
    "print('Finished building vocab')\n",
    "\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data), \n",
    "    batch_size = batch_size,\n",
    "    device = device,\n",
    "    sort_within_batch = True,\n",
    "    sort_key=lambda x: len(x.review)\n",
    ")\n",
    "\n",
    "test_iterator = data.Iterator(\n",
    "    test_data, \n",
    "    batch_size = batch_size, \n",
    "    device = device, \n",
    "    sort_within_batch = True, \n",
    "    sort_key=lambda x: len(x.review)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers, dropout, padding_idx):\n",
    "        super(BiLSTM_RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=True)\n",
    "        self.fc = nn.Linear(2*hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, review, review_length):        \n",
    "        embedded = self.dropout(self.embedding(review))\n",
    "        padded = nn.utils.rnn.pack_padded_sequence(embedded, review_length)\n",
    "        hidden = self.lstm(padded)[1][0]\n",
    "        cat = torch.cat((hidden[-2], hidden[-1]), dim = 1)\n",
    "        return self.fc(cat.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(REVIEW.vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 100\n",
    "output_dim = 5\n",
    "num_layers = 1\n",
    "dropout = 0.5\n",
    "padding_index = REVIEW.vocab.stoi['<pad>']\n",
    "unknown_index = REVIEW.vocab.stoi['<unk>']\n",
    "\n",
    "model = BiLSTM_RNN(vocab_size, embedding_dim, hidden_dim, output_dim, num_layers, dropout, padding_index)\n",
    "\n",
    "#Load pretrained embeddings\n",
    "pretrained_embeddings = REVIEW.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "#Reset unknown and padding vectors\n",
    "model.embedding.weight.data[unknown_index] = torch.zeros(embedding_dim, device=device)\n",
    "model.embedding.weight.data[padding_index] = torch.zeros(embedding_dim, device=device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, y):\n",
    "    num_correct = torch.sum(torch.argmax(preds, dim=1, keepdim=False) == y)\n",
    "    return num_correct.item()/len(y)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    loss, acc = 0, 0\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()   \n",
    "        preds = model(*batch.review)\n",
    "\n",
    "        batch_loss = criterion(preds, batch.score)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss += batch_loss.item()\n",
    "        acc += accuracy(preds, batch.score)\n",
    "        \n",
    "    return loss/len(iterator), acc/len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            preds = model(*batch.review)\n",
    "            batch_loss = criterion(preds, batch.score)\n",
    "            loss += batch_loss.item()\n",
    "            acc += accuracy(preds, batch.score)\n",
    "        \n",
    "    return loss/len(iterator), acc/len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only train for 5 epochs here, but I am able to get around 67% validation accuracy with 20 epochs. However, training accuracy surges beyond 80% which implies heavy over-fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 at 00:00:34\n",
      "Train Loss: 1.341 - Validation Loss: 1.126\n",
      "Train Acc: 0.40 - Validation Acc: 0.51\n",
      "\n",
      "Epoch 2 at 00:00:34\n",
      "Train Loss: 1.119 - Validation Loss: 1.062\n",
      "Train Acc: 0.52 - Validation Acc: 0.54\n",
      "\n",
      "Epoch 3 at 00:00:34\n",
      "Train Loss: 1.028 - Validation Loss: 0.993\n",
      "Train Acc: 0.56 - Validation Acc: 0.58\n",
      "\n",
      "Epoch 4 at 00:00:34\n",
      "Train Loss: 0.970 - Validation Loss: 0.971\n",
      "Train Acc: 0.59 - Validation Acc: 0.59\n",
      "\n",
      "Epoch 5 at 00:00:34\n",
      "Train Loss: 0.923 - Validation Loss: 0.933\n",
      "Train Acc: 0.61 - Validation Acc: 0.61\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "best_loss = np.inf\n",
    "\n",
    "for epoch in np.arange(epochs):\n",
    "    start = time.time()\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    end = time.time()\n",
    "\n",
    "    duration = time.strftime('%H:%M:%S', time.gmtime(end - start))\n",
    "\n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss\n",
    "        with open('model.pkl', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "    print(f'\\nEpoch {epoch + 1} at {duration}')\n",
    "    print(f'Train Loss: {train_loss:.3f} - Validation Loss: {valid_loss:.3f}')\n",
    "    print(f'Train Acc: {train_acc:.2f} - Validation Acc: {valid_acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.941\n",
      "Test Acc: 0.60%\n"
     ]
    }
   ],
   "source": [
    "with open('model.pkl','rb') as f:\n",
    "    model = pickle.load(f)\n",
    "model.to(device)\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f}')\n",
    "print(f'Test Acc: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(review, tokenizer, review_vocab, score_vocab, print_prob=False):\n",
    "    tokens = [t.text for t in tokenizer(review)]\n",
    "    indices = [review_vocab.stoi[t] for t in tokens]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input = torch.LongTensor(indices).to(device).unsqueeze(1)\n",
    "        likelihood = model(input, torch.tensor([len(tokens)]))\n",
    "        probs = torch.softmax(likelihood, 0)\n",
    "        pred_index = probs.argmax().item()\n",
    "        scores = np.array(score_vocab.itos[0:5]).astype(int)\n",
    "        prediction = score_vocab.itos[pred_index]\n",
    "        \n",
    "        if print_prob:\n",
    "            print('Probability distribution: ', probs[np.argsort(scores)].cpu().numpy())\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability distribution:  [0.00427912 0.00304541 0.01441015 0.19444345 0.7838218 ]\n",
      "Predicted score: 5\n"
     ]
    }
   ],
   "source": [
    "prediction = predict(\"The way it tastes is great. Absolutely amazing.\", nlp.tokenizer, REVIEW.vocab, SCORE.vocab, print_prob=True)\n",
    "print(f'Predicted score: {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability distribution:  [0.03027532 0.15796894 0.72844136 0.07762711 0.00568723]\n",
      "Predicted score: 3\n"
     ]
    }
   ],
   "source": [
    "prediction = predict(\"It was okay, but nothing fancy.\", nlp.tokenizer, REVIEW.vocab, SCORE.vocab, print_prob=True)\n",
    "print(f'Predicted score: {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability distribution:  [0.8194858  0.15229692 0.02525269 0.00197759 0.00098703]\n",
      "Predicted score: 1\n"
     ]
    }
   ],
   "source": [
    "prediction = predict(\"Very disappointing from such a reputable company\", nlp.tokenizer, REVIEW.vocab, SCORE.vocab, print_prob=True)\n",
    "print(f'Predicted score: {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewdf = pd.read_csv(\"./data/Reviews.csv\", usecols=[\"Score\",\"Text\"])\n",
    "reviewdf[\"Predicted_score\"] = reviewdf[\"Text\"].apply(lambda x: predict(x, SCORE.vocab))\n",
    "reviewdf[\"Predicted_score\"] = pd.to_numeric(reviewdf[\"Predicted_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix indicates that our model is really adept at predicting 1-star ratings and 5-star ratings, but starts to get ambiguous at the 2, 3, 4 star ratings. This makes sense as there is a lot of room for slack. How different is a 2 from a 3? A 4 is also really close to a 5 and can be really difficult to pickup on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 44957,   2393,   1541,    379,   2998],\n",
       "       [ 15492,   6595,   4143,   1039,   2500],\n",
       "       [  9449,   4380,  15588,   6870,   6353],\n",
       "       [  5317,    935,   9056,  27402,  37945],\n",
       "       [ 16185,   1032,   6567,  32231, 307107]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, mean_absolute_error\n",
    "\n",
    "cm = confusion_matrix(reviewdf[\"Score\"], reviewdf[\"Predicted_score\"])\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7065637676927244"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(reviewdf[\"Score\"], reviewdf[\"Predicted_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6995135110714451"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(reviewdf[\"Score\"], reviewdf[\"Predicted_score\"], average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, this is how much we would be off by if we predicted the mean every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0435269810958099"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(reviewdf[\"Score\"], np.repeat(np.mean(reviewdf[\"Score\"]), len(reviewdf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model predictions are off by less than 0.5 stars on average. This is really good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.472675009763323"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(reviewdf[\"Score\"], reviewdf[\"Predicted_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original distribution of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.091948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.052368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.075010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.141885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.638789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Text\n",
       "Score          \n",
       "1      0.091948\n",
       "2      0.052368\n",
       "3      0.075010\n",
       "4      0.141885\n",
       "5      0.638789"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewdf.groupby(\"Score\").count()[[\"Text\"]]/len(reviewdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (anaconda3 2018.12)",
   "language": "python",
   "name": "anaconda3-2018.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
